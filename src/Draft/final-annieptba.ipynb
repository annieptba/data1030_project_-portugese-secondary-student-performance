{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# 5. Classification\n",
    "**Use Students' Information To Predict Their Final Grade**\n",
    "\"\"\"\n",
    "# create dataframe dfd for classification\n",
    "dfd = df.copy()\n",
    "dfd = dfd.drop([ 'final_score'], axis=1)\n",
    "\"\"\"# 5.1 Prepare Dataset for Modelling\"\"\"\n",
    "# label encode final_grade\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "dfd.final_grade = le.fit_transform(dfd.final_grade)\n",
    "# dataset train_test_split\n",
    "from sklearn.model_selection  import train_test_split\n",
    "X = dfd.drop('final_grade',axis=1)\n",
    "y = dfd.final_grade\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n",
    "# get dummy varibles \n",
    "X_train = pd.get_dummies(X_train)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "# see total number of features\n",
    "len(list(X_train))\n",
    "# see total number of features\n",
    "len(list(y_train))\n",
    "# see total number of features\n",
    "len(list(X_test))\n",
    "# see total number of features\n",
    "len(list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"# 5.2 Decision Tree Classification\"\"\"\n",
    "# find the optimal # of minimum samples leaf\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "msl=[]\n",
    "for i in range(1,58):\n",
    "tree = DecisionTreeClassifier(min_samples_leaf=i)\n",
    "t= tree.fit(X_train, y_train)\n",
    "ts=t.score(X_test, y_test)\n",
    "msl.append(ts)\n",
    "msl = pd.Series(msl)\n",
    "max_msl = msl.where(msl==msl.max()).dropna()\n",
    "print(max_msl)\n",
    "index_max_msl = max_msl.index[0] # Get index you need\n",
    "print(index_max_msl)\n",
    "# final model\n",
    "print('Index max msl: ',index_max_msl)\n",
    "tree = DecisionTreeClassifier(min_samples_leaf=index_max_msl)\n",
    "t= tree.fit(X_train, y_train)\n",
    "print(\"Decisioin Tree Model Score\" , \":\" , t.score(X_train, y_train) , \",\" , \n",
    "\"Cross Validation Score\" ,\":\" , t.score(X_test, y_test))\n",
    "dtc_score = np.zeros(2)\n",
    "dtc_score[0] = t.score(X_train, y_train)\n",
    "dtc_score[1] = t.score(X_test, y_test)\n",
    "dtc_score\n",
    "\"\"\"# 5.3 Random Forest Classification\"\"\"\n",
    "# find a good # of estimators\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "ne=[]\n",
    "for i in range(1,58):\n",
    "forest = RandomForestClassifier()\n",
    "f = forest.fit(X_train, y_train)\n",
    "fs = f.score(X_test, y_test)\n",
    "ne.append(fs)\n",
    "ne = pd.Series(ne)\n",
    "max_ne = ne.where(ne==ne.max()).dropna()\n",
    "index_max_ne_1 = max_ne.index[0]\n",
    "print(index_max_ne_1)\n",
    "# find a good # of min_samples_leaf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "ne=[]\n",
    "for i in range(1,58):\n",
    "forest = RandomForestClassifier(n_estimators=index_max_ne_1, min_samples_leaf=i)\n",
    "f = forest.fit(X_train, y_train)\n",
    "fs = f.score(X_test, y_test)\n",
    "ne.append(fs)\n",
    "ne = pd.Series(ne)\n",
    "max_ne = ne.where(ne==ne.max()).dropna()\n",
    "index_max_ne_2 = max_ne.index[0]\n",
    "print(max_ne)\n",
    "print(index_max_ne_2)\n",
    "# final model\n",
    "print('Index max ne: ',index_max_ne_1,index_max_ne_2)\n",
    "forest = RandomForestClassifier(n_estimators=index_max_ne_1, min_samples_leaf=index_max_ne_2)\n",
    "f = forest.fit(X_train, y_train)\n",
    "print(\"Random Forest Model Score\" , \":\" , f.score(X_train, y_train) , \",\" ,\n",
    "\"Cross Validation Score\" ,\":\" , f.score(X_test, y_test))\n",
    "rfc_score = np.zeros(2)\n",
    "rfc_score[0] = (f.score(X_train, y_train))\n",
    "rfc_score[1] = (f.score(X_test, y_test))\n",
    "\"\"\"# 5.4 Support Vector Classification\"\"\"\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "s= svc.fit(X_train, y_train)\n",
    "print(\"SVC Model Score\" , \":\" , s.score(X_train, y_train) , \",\" ,\n",
    "\"Cross Validation Score\" ,\":\" , s.score(X_test, y_test))\n",
    "svc_score = np.zeros(2)\n",
    "svc_score[0] = (s.score(X_train, y_train))\n",
    "svc_score[1] = (s.score(X_test,y_test))\n",
    "\"\"\"# 5.5 Logistic Regression\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(multi_class='multinomial', solver='newton-cg',fit_intercept=True)\n",
    "# find optimal # of features to use in the model\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "ks=[]\n",
    "for i in range(1,58):\n",
    "sk = SelectKBest(chi2, k=i)\n",
    "x_new = sk.fit_transform(X_train,y_train)\n",
    "x_new_test=sk.fit_transform(X_test,y_test)\n",
    "l = lr.fit(x_new, y_train)\n",
    "ll = l.score(x_new_test, y_test)\n",
    "ks.append(ll)  \n",
    "ks = pd.Series(ks)\n",
    "ks = ks.reindex(list(range(1,58)))\n",
    "max_ks = ks.where(ks==ks.max()).dropna()\n",
    "index_max_ks = max_ks.index[0]\n",
    "print(max_ks)\n",
    "print(index_max_ks)\n",
    "plt.figure(figsize=(10,5))\n",
    "ks.plot.line()\n",
    "plt.title('Feature Selction', fontsize=20)\n",
    "plt.xlabel('Number of Feature Used', fontsize=16)\n",
    "plt.ylabel('Prediction Accuracy', fontsize=16)\n",
    "# final model\n",
    "print('Index max ks: ', index_max_ks)\n",
    "sk = SelectKBest(chi2, k=index_max_ks)\n",
    "x_new = sk.fit_transform(X_train,y_train)\n",
    "x_new_test=sk.fit_transform(X_test,y_test)\n",
    "lr = lr.fit(x_new, y_train)\n",
    "print(\"Logistic Regression Model Score\" , \":\" , lr.score(x_new, y_train) , \",\" ,\n",
    "\"Cross Validation Score\" ,\":\" , lr.score(x_new_test, y_test))\n",
    "lrc_score = np.zeros(2)\n",
    "lrc_score[0] = (lr.score(x_new, y_train))\n",
    "lrc_score[1] = (lr.score(x_new_test, y_test))\n",
    "\"\"\"#5.6 Ada Boost Classification\"\"\"\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(n_estimators=2)\n",
    "af = ada.fit(X_train, y_train)\n",
    "print(\"Ada Boost Model Score\" , \":\" , af.score(X_train, y_train) , \",\" ,\n",
    "\"Cross Validation Score\" ,\":\" , af.score(X_test, y_test))\n",
    "abc_score = np.zeros(2)\n",
    "abc_score[0] = (af.score(X_train, y_train))\n",
    "abc_score[1] = (af.score(X_test, y_test))\n",
    "\"\"\"# 5.7 Stochastic Gradient Descent Classification\"\"\"\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier()\n",
    "sf = sgd.fit(X_train, y_train)\n",
    "print(\"Stochastic Gradient Descent Model Score\" , \":\" , sf.score(X_train, y_train) , \",\" ,\n",
    "\"Cross Validation Score\" ,\":\" , sf.score(X_test, y_test))\n",
    "sgc_score = np.zeros(2)\n",
    "sgc_score[0] = (sf.score(X_train, y_train))\n",
    "sgc_score[1] = (sf.score(X_test, y_test))\n",
    "\"\"\"# 5.8 Model Selection\n",
    "Let's compare the performance of each model!\n",
    "\"\"\"\n",
    "msm = np.array([dtc_score,rfc_score,svc_score,lrc_score,abc_score,sgc_score])\n",
    "msm = pd.DataFrame(msm,columns = ['model_score','validation_score'])\n",
    "model = np.array(['Decision tree','Random forest','Support vector','Logistic regression','Ada boost','Stochastic gradient descent'])\n",
    "model = pd.DataFrame(model,columns = ['classification'])\n",
    "msm = pd.DataFrame([model.classification,msm.model_score,msm.validation_score])\n",
    "msm = msm.T\n",
    "msm\n",
    "print('We choose this model: ')\n",
    "max_msm = msm.where(msm.validation_score == msm.validation_score.max()).dropna()\n",
    "max_msm\n",
    "\"\"\"# 6. Summary\n",
    "**The valedictorian of the high school class is likely to have this profile:**\n",
    " \n",
    "*   Is not in a romantic relationship\n",
    "*   Does not consume alcohol\n",
    "*   Living  in urban area\n",
    "*   Does not go out with friends frequently\n",
    "*   Have strong desire of receiving higher education\n",
    "*   Parents both received higher education\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
